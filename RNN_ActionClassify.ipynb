{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification using Recurrent Neural Networks(RNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as DD\n",
    "import torchnet as tnt\n",
    "import torch.optim as optim\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use cuda: %s'%(use_cuda))\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The data we are using is skeleton data, which indicates the 3D locations of body joints. In total, there are 25 body joints. It is collected by Kinect v2. To make it easier, each sequence have same number of frames. You need to classify 10 different actions. There are 2000 training sequences, 400 validation sequences, and 500 test sequences. Each sequence has 15 frames, each frame is a 75-dimension vector (3*25).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(DD.Dataset):\n",
    "    # subset can be: 'train', 'val', 'test'\n",
    "    def __init__(self, data_path, subset='train'):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_path = os.path.join(data_path, '%s_data.h5'%subset)\n",
    "        self.subset = subset\n",
    "\n",
    "        with h5py.File(self.data_path) as f:\n",
    "            self.data = np.array(f['data'])\n",
    "\n",
    "        if subset != 'test':\n",
    "            self.label_path = os.path.join(data_path, '%s_label.h5'%subset)\n",
    "            with h5py.File(self.label_path) as f:\n",
    "                self.label = np.array(f['label'])\n",
    "\n",
    "        self.num_sequences = self.data.shape[0]\n",
    "        self.seq_len = self.data.shape[1]\n",
    "        self.n_dim = self.data.shape[2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.data[index]\n",
    "        if self.subset != 'test':\n",
    "            label = int(self.label[index])\n",
    "            sample = {'seq': seq, 'label': label}\n",
    "        else:\n",
    "            sample = {'seq': seq}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_sequences\n",
    "\n",
    "trSet = Dataset('./data', subset='train')\n",
    "valSet = Dataset('./data', subset='val')\n",
    "tstSet = Dataset('./data', subset='test')\n",
    "\n",
    "batch_size = 50\n",
    "trLD = DD.DataLoader(trSet, batch_size=batch_size,\n",
    "       sampler=DD.sampler.RandomSampler(trSet),\n",
    "       num_workers=2, pin_memory=False)\n",
    "valLD = DD.DataLoader(valSet, batch_size=batch_size,\n",
    "       sampler=DD.sampler.SequentialSampler(valSet),\n",
    "       num_workers=1, pin_memory=False)\n",
    "tstLD = DD.DataLoader(tstSet, batch_size=batch_size,\n",
    "       sampler=DD.sampler.SequentialSampler(tstSet),\n",
    "       num_workers=1, pin_memory=False)\n",
    "\n",
    "input_dim = trSet.n_dim\n",
    "num_class = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence classification model\n",
    "class SequenceClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequenceClassify, self).__init__()\n",
    "        \n",
    "    \n",
    "        self.project_layer = nn.Linear(75, 100)\n",
    "        self.recurrent_layer = nn.LSTM(100, 100, num_layers=1)\n",
    "        self.classify_layer = nn.Linear(100, 10)\n",
    "      \n",
    "    \n",
    "    # the size of input is [batch_size, seq_len(15), input_dim(75)]\n",
    "    # the size of logits is [batch_size, num_class]\n",
    "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "        rnn_outputs, (hn, cn) = self.recurrent_layer(self.project_layer(input))\n",
    "        logits = self.classify_layer(rnn_outputs[:,-1])\n",
    "        return logits\n",
    "\n",
    "model = SequenceClassify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "After you have the dataloader and model, you can start training the model. Define a SGD optimizer with learning rate of 1e-3, and a cross-entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.cuda() # For GPU use\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 0  , Loss: 2.3062,  Accuracy: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h4k3r/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Epoch: 0  , Loss: 2.3045,  Accuracy: 9.75\n",
      "train Epoch: 1  , Loss: 2.3059,  Accuracy: 10.15\n",
      "train Epoch: 2  , Loss: 2.3049,  Accuracy: 9.90\n",
      "train Epoch: 3  , Loss: 2.3046,  Accuracy: 9.65\n",
      "train Epoch: 4  , Loss: 2.3042,  Accuracy: 10.05\n",
      "train Epoch: 5  , Loss: 2.3035,  Accuracy: 10.30\n",
      "val Epoch: 5  , Loss: 2.3025,  Accuracy: 9.75\n",
      "train Epoch: 6  , Loss: 2.3038,  Accuracy: 10.50\n",
      "train Epoch: 7  , Loss: 2.3032,  Accuracy: 10.00\n",
      "train Epoch: 8  , Loss: 2.3031,  Accuracy: 10.05\n",
      "train Epoch: 9  , Loss: 2.3024,  Accuracy: 10.35\n",
      "train Epoch: 10  , Loss: 2.3030,  Accuracy: 9.95\n",
      "val Epoch: 10  , Loss: 2.3016,  Accuracy: 10.00\n",
      "train Epoch: 11  , Loss: 2.3025,  Accuracy: 10.15\n",
      "train Epoch: 12  , Loss: 2.3022,  Accuracy: 10.10\n",
      "train Epoch: 13  , Loss: 2.3023,  Accuracy: 10.15\n",
      "train Epoch: 14  , Loss: 2.3022,  Accuracy: 10.25\n",
      "train Epoch: 15  , Loss: 2.3017,  Accuracy: 10.65\n",
      "val Epoch: 15  , Loss: 2.3011,  Accuracy: 11.00\n",
      "train Epoch: 16  , Loss: 2.3019,  Accuracy: 10.80\n",
      "train Epoch: 17  , Loss: 2.3015,  Accuracy: 11.10\n",
      "train Epoch: 18  , Loss: 2.3015,  Accuracy: 10.75\n",
      "train Epoch: 19  , Loss: 2.3013,  Accuracy: 10.85\n",
      "train Epoch: 20  , Loss: 2.3014,  Accuracy: 10.70\n",
      "val Epoch: 20  , Loss: 2.3008,  Accuracy: 11.75\n",
      "train Epoch: 21  , Loss: 2.3014,  Accuracy: 11.00\n",
      "train Epoch: 22  , Loss: 2.3010,  Accuracy: 11.85\n",
      "train Epoch: 23  , Loss: 2.3010,  Accuracy: 11.45\n",
      "train Epoch: 24  , Loss: 2.3010,  Accuracy: 12.40\n",
      "train Epoch: 25  , Loss: 2.3009,  Accuracy: 10.55\n",
      "val Epoch: 25  , Loss: 2.3005,  Accuracy: 13.25\n",
      "train Epoch: 26  , Loss: 2.3011,  Accuracy: 11.65\n",
      "train Epoch: 27  , Loss: 2.3011,  Accuracy: 12.40\n",
      "train Epoch: 28  , Loss: 2.3008,  Accuracy: 12.65\n",
      "train Epoch: 29  , Loss: 2.3007,  Accuracy: 13.10\n",
      "train Epoch: 30  , Loss: 2.3006,  Accuracy: 13.25\n",
      "val Epoch: 30  , Loss: 2.3002,  Accuracy: 12.75\n",
      "train Epoch: 31  , Loss: 2.3006,  Accuracy: 10.55\n",
      "train Epoch: 32  , Loss: 2.3005,  Accuracy: 12.35\n",
      "train Epoch: 33  , Loss: 2.3006,  Accuracy: 11.40\n",
      "train Epoch: 34  , Loss: 2.3006,  Accuracy: 12.35\n",
      "train Epoch: 35  , Loss: 2.3004,  Accuracy: 11.95\n",
      "val Epoch: 35  , Loss: 2.2999,  Accuracy: 13.00\n",
      "train Epoch: 36  , Loss: 2.3009,  Accuracy: 11.30\n",
      "train Epoch: 37  , Loss: 2.3005,  Accuracy: 12.80\n",
      "train Epoch: 38  , Loss: 2.3001,  Accuracy: 12.45\n",
      "train Epoch: 39  , Loss: 2.3005,  Accuracy: 12.40\n",
      "train Epoch: 40  , Loss: 2.3001,  Accuracy: 12.35\n",
      "val Epoch: 40  , Loss: 2.2997,  Accuracy: 14.00\n",
      "train Epoch: 41  , Loss: 2.3001,  Accuracy: 12.80\n",
      "train Epoch: 42  , Loss: 2.3003,  Accuracy: 12.30\n",
      "train Epoch: 43  , Loss: 2.3005,  Accuracy: 12.95\n",
      "train Epoch: 44  , Loss: 2.3000,  Accuracy: 13.55\n",
      "train Epoch: 45  , Loss: 2.3001,  Accuracy: 13.40\n",
      "val Epoch: 45  , Loss: 2.2995,  Accuracy: 14.00\n",
      "train Epoch: 46  , Loss: 2.3000,  Accuracy: 14.05\n",
      "train Epoch: 47  , Loss: 2.2998,  Accuracy: 14.25\n",
      "train Epoch: 48  , Loss: 2.3001,  Accuracy: 13.20\n",
      "train Epoch: 49  , Loss: 2.3003,  Accuracy: 13.05\n"
     ]
    }
   ],
   "source": [
    "# run the model for one epoch\n",
    "# can be used for both training or validation model\n",
    "def run_epoch(data_loader, model, criterion, epoch, is_training, optimizer=None):\n",
    "    if is_training:\n",
    "        model.train()\n",
    "        logger_prefix = 'train'\n",
    "    else:\n",
    "        model.eval()\n",
    "        logger_prefix = 'val'\n",
    "\n",
    "    confusion_matrix = tnt.meter.ConfusionMeter(num_class)\n",
    "    acc = tnt.meter.ClassErrorMeter(accuracy=True)\n",
    "    meter_loss = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for batch_idx, sample in enumerate(data_loader):\n",
    "        sequence = sample['seq']\n",
    "        label = sample['label']\n",
    "        input_sequence_var = Variable(sequence).type(FloatTensor)\n",
    "        input_label_var = Variable(label).type(LongTensor)\n",
    "\n",
    "        # compute output\n",
    "        # output_logits: [batch_size, num_class]\n",
    "        output_logits = model(input_sequence_var)\n",
    "        loss = criterion(output_logits, input_label_var)\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        meter_loss.add(loss.data[0])\n",
    "        acc.add(output_logits.data, input_label_var.data)\n",
    "        confusion_matrix.add(output_logits.data, input_label_var.data)\n",
    "\n",
    "\n",
    "    print('%s Epoch: %d  , Loss: %.4f,  Accuracy: %.2f'%(logger_prefix, epoch, meter_loss.value()[0], acc.value()[0]))\n",
    "    return acc.value()[0]\n",
    "\n",
    "num_epochs = 50\n",
    "evaluate_every_epoch = 5\n",
    "for e in range(num_epochs):\n",
    "    run_epoch(trLD, model, criterion, e, True, optimizer)\n",
    "    if e % evaluate_every_epoch == 0:\n",
    "        run_epoch(valLD, model, criterion, e, False, None)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence classification model\n",
    "class SequenceClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequenceClassify, self).__init__()\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.project_layer = nn.Linear(75,100)\n",
    "        self.recurrent_layer = nn.LSTM(100, 200, num_layers=2, dropout=0.5, batch_first = True)\n",
    "        self.recurrent_layer2 = nn.LSTM(200, 400, num_layers=4, dropout=0.5, batch_first = True)\n",
    "        self.classify_layer1 = nn.Linear(400, 20)\n",
    "        self.classify_layer2 = nn.Linear(20, 10)\n",
    "       \n",
    "        \n",
    "\n",
    "    # the size of input is [batch_size, seq_len(15), input_dim(75)]\n",
    "    # the size of logits is [batch_size, num_class]\n",
    "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "        # the size of rnn_outputs is [batch_size, seq_len, rnn_size]\n",
    "        rnn_outputs, (hn, cn) = self.recurrent_layer(self.project_layer(input))\n",
    "        rnn_outputs, (hn, cn) = self.recurrent_layer2(rnn_outputs)\n",
    "        logits = self.classify_layer1(rnn_outputs[:,-1])\n",
    "        logits = self.classify_layer2(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h4k3r/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch: 0  , Loss: 2.3112,  Accuracy: 9.85\n",
      "val Epoch: 0  , Loss: 2.2919,  Accuracy: 12.00\n",
      "train Epoch: 1  , Loss: 2.3059,  Accuracy: 10.10\n",
      "train Epoch: 2  , Loss: 2.3042,  Accuracy: 9.25\n",
      "train Epoch: 3  , Loss: 2.2992,  Accuracy: 10.15\n",
      "train Epoch: 4  , Loss: 2.1516,  Accuracy: 18.05\n",
      "train Epoch: 5  , Loss: 1.8200,  Accuracy: 29.75\n",
      "val Epoch: 5  , Loss: 1.7288,  Accuracy: 30.25\n",
      "train Epoch: 6  , Loss: 1.6123,  Accuracy: 39.80\n",
      "train Epoch: 7  , Loss: 1.5043,  Accuracy: 43.95\n",
      "train Epoch: 8  , Loss: 1.4577,  Accuracy: 46.55\n",
      "train Epoch: 9  , Loss: 1.3615,  Accuracy: 49.90\n",
      "train Epoch: 10  , Loss: 1.2096,  Accuracy: 56.25\n",
      "val Epoch: 10  , Loss: 1.1239,  Accuracy: 60.50\n",
      "train Epoch: 11  , Loss: 1.1571,  Accuracy: 57.60\n",
      "train Epoch: 12  , Loss: 1.1577,  Accuracy: 57.70\n",
      "train Epoch: 13  , Loss: 1.0083,  Accuracy: 63.25\n",
      "train Epoch: 14  , Loss: 1.0049,  Accuracy: 63.85\n",
      "train Epoch: 15  , Loss: 1.0283,  Accuracy: 62.20\n",
      "val Epoch: 15  , Loss: 1.1155,  Accuracy: 61.75\n",
      "train Epoch: 16  , Loss: 0.9216,  Accuracy: 66.05\n",
      "train Epoch: 17  , Loss: 0.8874,  Accuracy: 66.50\n",
      "train Epoch: 18  , Loss: 0.8546,  Accuracy: 68.10\n",
      "train Epoch: 19  , Loss: 0.9357,  Accuracy: 64.80\n",
      "train Epoch: 20  , Loss: 0.9098,  Accuracy: 66.25\n",
      "val Epoch: 20  , Loss: 0.9303,  Accuracy: 68.50\n",
      "train Epoch: 21  , Loss: 0.8467,  Accuracy: 68.65\n",
      "train Epoch: 22  , Loss: 0.7507,  Accuracy: 71.80\n",
      "train Epoch: 23  , Loss: 0.7552,  Accuracy: 71.70\n",
      "train Epoch: 24  , Loss: 0.7178,  Accuracy: 73.45\n",
      "train Epoch: 25  , Loss: 0.7116,  Accuracy: 74.05\n",
      "val Epoch: 25  , Loss: 0.8468,  Accuracy: 72.75\n",
      "train Epoch: 26  , Loss: 0.6182,  Accuracy: 78.15\n",
      "train Epoch: 27  , Loss: 0.6436,  Accuracy: 75.65\n",
      "train Epoch: 28  , Loss: 0.5956,  Accuracy: 79.30\n",
      "train Epoch: 29  , Loss: 0.5682,  Accuracy: 79.80\n",
      "train Epoch: 30  , Loss: 0.5129,  Accuracy: 80.65\n",
      "val Epoch: 30  , Loss: 0.9323,  Accuracy: 76.25\n",
      "train Epoch: 31  , Loss: 0.5330,  Accuracy: 80.70\n",
      "train Epoch: 32  , Loss: 0.4923,  Accuracy: 81.95\n",
      "train Epoch: 33  , Loss: 0.4690,  Accuracy: 83.40\n",
      "train Epoch: 34  , Loss: 0.4826,  Accuracy: 81.85\n",
      "train Epoch: 35  , Loss: 0.4446,  Accuracy: 83.20\n",
      "val Epoch: 35  , Loss: 0.8167,  Accuracy: 76.50\n",
      "train Epoch: 36  , Loss: 0.4975,  Accuracy: 81.85\n",
      "train Epoch: 37  , Loss: 0.5199,  Accuracy: 81.00\n",
      "train Epoch: 38  , Loss: 0.4033,  Accuracy: 85.25\n",
      "train Epoch: 39  , Loss: 0.3866,  Accuracy: 85.90\n",
      "train Epoch: 40  , Loss: 0.3550,  Accuracy: 86.90\n",
      "val Epoch: 40  , Loss: 0.7804,  Accuracy: 80.00\n",
      "train Epoch: 41  , Loss: 0.3527,  Accuracy: 87.30\n",
      "train Epoch: 42  , Loss: 0.3731,  Accuracy: 85.45\n",
      "train Epoch: 43  , Loss: 0.3956,  Accuracy: 85.90\n",
      "train Epoch: 44  , Loss: 0.3618,  Accuracy: 86.55\n",
      "train Epoch: 45  , Loss: 0.3284,  Accuracy: 86.85\n",
      "val Epoch: 45  , Loss: 0.8918,  Accuracy: 77.50\n",
      "train Epoch: 46  , Loss: 0.3520,  Accuracy: 86.65\n",
      "train Epoch: 47  , Loss: 0.2778,  Accuracy: 89.30\n",
      "train Epoch: 48  , Loss: 0.3406,  Accuracy: 87.95\n",
      "train Epoch: 49  , Loss: 0.4045,  Accuracy: 85.85\n",
      "train Epoch: 50  , Loss: 0.3424,  Accuracy: 87.80\n",
      "val Epoch: 50  , Loss: 0.8318,  Accuracy: 78.75\n",
      "train Epoch: 51  , Loss: 0.2860,  Accuracy: 89.15\n",
      "train Epoch: 52  , Loss: 0.2715,  Accuracy: 89.95\n",
      "train Epoch: 53  , Loss: 0.2367,  Accuracy: 91.05\n",
      "train Epoch: 54  , Loss: 0.2550,  Accuracy: 90.80\n",
      "train Epoch: 55  , Loss: 0.2692,  Accuracy: 90.30\n",
      "val Epoch: 55  , Loss: 0.9464,  Accuracy: 77.00\n",
      "train Epoch: 56  , Loss: 0.2443,  Accuracy: 90.35\n",
      "train Epoch: 57  , Loss: 0.2790,  Accuracy: 90.00\n",
      "train Epoch: 58  , Loss: 0.2469,  Accuracy: 91.10\n",
      "train Epoch: 59  , Loss: 0.1875,  Accuracy: 93.05\n",
      "train Epoch: 60  , Loss: 0.1858,  Accuracy: 93.25\n",
      "val Epoch: 60  , Loss: 0.8580,  Accuracy: 80.25\n",
      "train Epoch: 61  , Loss: 0.1968,  Accuracy: 93.25\n",
      "train Epoch: 62  , Loss: 0.2290,  Accuracy: 91.45\n",
      "train Epoch: 63  , Loss: 0.2281,  Accuracy: 91.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-257:\n",
      "Process Process-256:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/h4k3r/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-98249488ad09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mevaluate_every_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluate_every_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-98249488ad09>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_loader, model, criterion, epoch, is_training, optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmeter_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_label_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_label_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchnet/meter/averagevaluemeter.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, value, n)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_s\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_s\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = SequenceClassify()\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# run the model for one epoch\n",
    "# can be used for both training or validation model\n",
    "def run_epoch(data_loader, model, criterion, epoch, is_training, optimizer=None):\n",
    "    if is_training:\n",
    "        model.train()\n",
    "        logger_prefix = 'train'\n",
    "    else:\n",
    "        model.eval()\n",
    "        logger_prefix = 'val'\n",
    "\n",
    "    confusion_matrix = tnt.meter.ConfusionMeter(num_class)\n",
    "    acc = tnt.meter.ClassErrorMeter(accuracy=True)\n",
    "    meter_loss = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for batch_idx, sample in enumerate(data_loader):\n",
    "        sequence = sample['seq']\n",
    "        label = sample['label']\n",
    "        input_sequence_var = Variable(sequence).type(FloatTensor)\n",
    "        input_label_var = Variable(label).type(LongTensor)\n",
    "\n",
    "        # compute output\n",
    "        # output_logits: [batch_size, num_class]\n",
    "        output_logits = model(input_sequence_var)\n",
    "        loss = criterion(output_logits, input_label_var)\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        meter_loss.add(loss.data[0])\n",
    "        acc.add(output_logits.data, input_label_var.data)\n",
    "        confusion_matrix.add(output_logits.data, input_label_var.data)\n",
    "\n",
    "\n",
    "    print('%s Epoch: %d  , Loss: %.4f,  Accuracy: %.2f'%(logger_prefix, epoch, meter_loss.value()[0], acc.value()[0]))\n",
    "    return acc.value()[0]\n",
    "\n",
    "num_epochs = 1000\n",
    "evaluate_every_epoch = 5\n",
    "for e in range(num_epochs):\n",
    "    run_epoch(trLD, model, criterion, e, True, optimizer)\n",
    "    if e % evaluate_every_epoch == 0:\n",
    "        run_epoch(valLD, model, criterion, e, False, None)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your best model to generate results on test set.\n",
    "\n",
    "# generate csv file for test set\n",
    "def predict_on_test(model, data_loader):\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    results=open('resultsss.csv','w')\n",
    "    count=0\n",
    "    results.write('Id'+','+'Class'+'\\n')\n",
    "    for batch_idx, sample in enumerate(data_loader):\n",
    "        sequence = sample['seq']\n",
    "        input_sequence_var = Variable(sequence).type(FloatTensor)\n",
    "        scores = model(input_sequence_var)\n",
    "        _, preds = scores.data.max(1)\n",
    "        for i in range(len(preds)):\n",
    "            results.write(str(count)+','+str(preds[i])+'\\n')\n",
    "            count+=1\n",
    "    results.close()\n",
    "    return count\n",
    "\n",
    "count=predict_on_test(model, tstLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
